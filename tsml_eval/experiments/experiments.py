# -*- coding: utf-8 -*-
"""Functions to perform classification and clustering experiments.

Results are saved a standardised format used by both tsml and sktime.
"""

__author__ = ["TonyBagnall", "MatthewMiddlehurst"]


import os
import time
import warnings
from datetime import datetime

import numpy as np
import pandas as pd
from sklearn import preprocessing
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.model_selection import cross_val_predict
from sktime.datasets import load_from_tsfile_to_dataframe as load_ts

from tsml_eval.evaluation.metrics import clustering_accuracy
from tsml_eval.utils.experiments import (
    resample_data,
    stratified_resample_data,
    write_classification_results,
    write_clustering_results,
    write_regression_results,
)


def run_classification_experiment(
    X_train,
    y_train,
    X_test,
    y_test,
    classifier,
    results_path,
    classifier_name="",
    dataset_name="",
    resample_id=None,
    build_test_file=True,
    build_train_file=False,
):
    """Run a classification experiment and save the results to file.

    Method to run a basic experiment and write the results to files called
    testFold<resampleID>.csv and, if required, trainFold<resampleID>.csv.

    Parameters
    ----------
    X_train : pd.DataFrame or np.array
        The data to train the classifier.
    y_train : np.array, default = None
        Training data class labels.
    X_test : pd.DataFrame or np.array, default = None
        The data used to test the trained classifier.
    y_test : np.array, default = None
        Testing data class labels.
    classifier : BaseClassifier
        Classifier to be used in the experiment.
    results_path : str
        Location of where to write results. Any required directories will be created.
    classifier_name : str, default=""
        Name of the classifier.
    dataset_name : str, default=""
        Name of problem.
    resample_id : int, default=0
        Seed for resampling. If set to 0, the default train/test split from file is
        used. Also used in output file name.
    build_train_file : bool, default=False
        Whether to generate train files or not. If true, it performs a 10-fold
        cross-validation on the train data and saves. If the classifier can produce its
        own estimates, those are used instead.
    build_test_file : bool, default=True:
         Whether to generate test files or not. If the classifier can generate its own
         train probabilities, the classifier will be built but no file will be output.
    """
    if not build_test_file and not build_train_file:
        raise Exception(
            "Both test_file and train_file are set to False. "
            "At least one must be written."
        )

    le = preprocessing.LabelEncoder()
    y_train = le.fit_transform(y_train)
    y_test = le.transform(y_test)

    encoder_dict = {label: i for i, label in enumerate(le.classes_)}
    classifier_train_probs = build_train_file and callable(
        getattr(classifier, "_get_train_probs", None)
    )
    fit_time = -1

    first_comment = (
        "PREDICTIONS,Generated by classification_experiments.py on "
        f"{datetime.now().strftime('%m/%d/%Y, %H:%M:%S')}. "
        f"Encoder dictionary: {str(encoder_dict)}"
    )

    second = str(classifier.get_params())
    second.replace("\n", " ")
    second.replace("\r", " ")

    if build_test_file or classifier_train_probs:
        start = int(round(time.time() * 1000))
        classifier.fit(X_train, y_train)
        fit_time = int(round(time.time() * 1000)) - start

    if build_test_file:
        start = int(round(time.time() * 1000))
        test_probs = classifier.predict_proba(X_test)
        test_time = int(round(time.time() * 1000)) - start

        test_preds = classifier.classes_[np.argmax(test_probs, axis=1)]
        test_acc = accuracy_score(y_test, test_preds)
        third = (
            f"{test_acc},"  # 1. accuracy
            f"{fit_time},"  # 2. fit time
            f"{test_time},"  # 3. predict time
            "-1,-1,"  # 4. benchmark time, 5. memory usage
            f"{len(classifier.classes_)},"  # 6. number of classes
            ",-1,-1"  # 7. type of train estimate used, 8. estimate time
            # 9. build plus estimate time (all not relevant to test files)
        )

        write_classification_results(
            first_line_comment=first_comment,
            second_line=second,
            third_line=third,
            timing_type="MILLISECONDS",
            output_path=results_path,
            estimator_name=classifier_name,
            resample_id=resample_id,
            y_pred=test_preds,
            predicted_probs=test_probs,
            dataset_name=dataset_name,
            y_true=y_test,
            split="TEST",
            full_path=False,
        )

    if build_train_file:
        start = int(round(time.time() * 1000))
        if classifier_train_probs:  # Normally can only do this if test has been built
            train_probs = classifier._get_train_probs(X_train, y_train)
        else:
            cv_size = 10
            _, counts = np.unique(y_train, return_counts=True)
            min_class = np.min(counts)
            if min_class < cv_size:
                cv_size = min_class

            train_probs = cross_val_predict(
                classifier, X_train, y=y_train, cv=cv_size, method="predict_proba"
            )
        train_time = int(round(time.time() * 1000)) - start

        train_preds = classifier.classes_[np.argmax(train_probs, axis=1)]
        train_acc = accuracy_score(y_train, train_preds)
        third = (
            f"{train_acc},"  # 1. accuracy
            f"{fit_time},-1,-1,-1,"  # 2. fit time
            # 3. predict time (not relevant for train files)
            # 4. benchmark time, 5. memory usage
            f"{len(classifier.classes_)},"  # 6. number of classes
            f",{train_time},"  # 7. type of train estimate used, 8. estimate time
            f"{fit_time + train_time}"  # 8. build plus estimate time
        )

        write_classification_results(
            first_line_comment=first_comment,
            second_line=second,
            third_line=third,
            timing_type="MILLISECONDS",
            output_path=results_path,
            estimator_name=classifier_name,
            resample_id=resample_id,
            y_pred=train_preds,
            predicted_probs=train_probs,
            dataset_name=dataset_name,
            y_true=y_train,
            split="TRAIN",
            full_path=False,
        )


def load_and_run_classification_experiment(
    problem_path,
    results_path,
    dataset,
    classifier,
    resample_id=0,
    classifier_name=None,
    overwrite=False,
    build_train_file=False,
    predefined_resample=False,
):
    """Load a dataset and run a classification experiment.

    Method to run a basic experiment and write the results to files called
    testFold<resampleID>.csv and, if required, trainFold<resampleID>.csv.

    Parameters
    ----------
    problem_path : str
        Location of problem files, full path.
    results_path : str
        Location of where to write results. Any required directories will be created.
    dataset : str
        Name of problem. Files must be  <problem_path>/<dataset>/<dataset>+"_TRAIN.ts",
        same for "_TEST".
    classifier : BaseClassifier
        Classifier to be used in the experiment, if none is provided one is selected
        using cls_name using resample_id as a seed.
    classifier_name : str, default = None
        Name of classifier used in writing results. If none the name is taken from
        the classifier
    resample_id : int, default=0
        Seed for resampling. If set to 0, the default train/test split from file is
        used. Also used in output file name.
    overwrite : bool, default=False
        If set to False, this will only build results if there is not a result file
        already present. If True, it will overwrite anything already there.
    build_train_file : bool, default=False
        Whether to generate train files or not. If true, it performs a 10-fold
        cross-validation on the train data and saves. If the classifier can produce its
        own estimates, those are used instead.
    predefined_resample : bool, default=False
        Read a predefined resample from file instead of performing a resample. If True
        the file format must include the resample_id at the end of the dataset name i.e.
        <problem_path>/<dataset>/<dataset>+<resample_id>+"_TRAIN.ts".
    """
    if classifier_name is None:
        classifier_name = type(classifier).__name__

    build_test_file, build_train_file = _check_existing_results(
        results_path,
        classifier_name,
        dataset,
        resample_id,
        overwrite,
        True,
        build_train_file,
    )

    if not build_test_file and not build_train_file:
        warnings.warn("All files exist and not overwriting, skipping.")
        return

    X_train, y_train, X_test, y_test, resample = _load_data(
        problem_path, dataset, resample_id, predefined_resample
    )

    if resample:
        X_train, y_train, X_test, y_test = stratified_resample_data(
            X_train, y_train, X_test, y_test, random_state=resample_id
        )

    run_classification_experiment(
        X_train,
        y_train,
        X_test,
        y_test,
        classifier,
        results_path,
        classifier_name=classifier_name,
        dataset_name=dataset,
        resample_id=resample_id,
        build_test_file=build_test_file,
        build_train_file=build_train_file,
    )


def run_regression_experiment(
    X_train,
    y_train,
    X_test,
    y_test,
    regressor,
    results_path,
    regressor_name="",
    dataset_name="",
    resample_id=None,
    build_test_file=True,
    build_train_file=False,
):
    """Run a regression experiment and save the results to file.

    Method to run a basic experiment and write the results to files called
    testFold<resampleID>.csv and, if required, trainFold<resampleID>.csv.

    Parameters
    ----------
    X_train : pd.DataFrame or np.array
        The data to train the classifier.
    y_train : np.array, default = None
        Training data class labels.
    X_test : pd.DataFrame or np.array, default = None
        The data used to test the trained classifier.
    y_test : np.array, default = None
        Testing data class labels.
    regressor : BaseRegressor
        Regressor to be used in the experiment.
    results_path : str
        Location of where to write results. Any required directories will be created.
    regressor_name : str, default=""
        Name of the Regressor to use in file writing.
    dataset_name : str, default=""
        Name of problem to use in file writing.
    resample_id : int, default=0
        Seed for resampling. If set to 0, the default train/test split from file is
        used. Also used in output file name.
    build_train_file : bool, default=False
        Whether to generate train files or not. If true, it performs a 10-fold
        cross-validation on the train data and saves. If the regressor can produce its
        own estimates, those are used instead.
    build_test_file : bool, default=True:
         Whether to generate test files or not. If the regressor can generate its own
         train probabilities, the classifier will be built but no file will be output.
    """
    if not build_test_file and not build_train_file:
        raise Exception(
            "Both test_file and train_file are set to False. "
            "At least one must be written."
        )

    regressor_train_preds = build_train_file and callable(
        getattr(regressor, "_get_train_preds", None)
    )
    fit_time = -1

    first_comment = (
        "Generated by regression_experiments.py on "
        f"{datetime.now().strftime('%m/%d/%Y, %H:%M:%S')}"
    )

    second = str(regressor.get_params())
    second.replace("\n", " ")
    second.replace("\r", " ")

    if build_test_file or regressor_train_preds:
        start = int(round(time.time() * 1000))
        regressor.fit(X_train, y_train)
        fit_time = int(round(time.time() * 1000)) - start

    if build_test_file:
        start = int(round(time.time() * 1000))
        test_preds = regressor.predict(X_test)
        test_time = int(round(time.time() * 1000)) - start

        test_mse = mean_squared_error(y_test, test_preds)
        third = (
            f"{test_mse},"  # 1. mean squared error
            f"{fit_time},"  # 2. fit time
            f"{test_time},"  # 3. predict time
            f"-1,-1,"  # 4. benchmark time, 5. memory usage
            f",-1,-1"  # 6. type of train estimate used, 7. estimate time
            # 8. build plus estimate time (all not relevant to test files)
        )

        write_regression_results(
            first_line_comment=first_comment,
            second_line=second,
            third_line=third,
            timing_type="MILLISECONDS",
            output_path=results_path,
            estimator_name=regressor_name,
            resample_id=resample_id,
            y_pred=test_preds,
            dataset_name=dataset_name,
            y_true=y_test,
            split="TEST",
            full_path=False,
        )

    if build_train_file:
        start = int(round(time.time() * 1000))
        if regressor_train_preds:  # Normally can only do this if test has been built
            train_preds = regressor._get_train_preds(X_train, y_train)
        else:
            cv_size = min(10, len(y_train))
            train_preds = cross_val_predict(regressor, X_train, y=y_train, cv=cv_size)
        train_time = int(round(time.time() * 1000)) - start

        train_mse = mean_squared_error(y_train, train_preds)
        third = (
            f"{train_mse},"  # 1. mean squared error
            f"{fit_time},-1,-1,-1,,"  # 2. fit time
            # 3. predict time (not relevant for train files)
            # 4. benchmark time, 5. memory usage
            # 6. type of train estimate used
            f"{train_time},"  # 7. estimate time
            f"{fit_time + train_time}"  # 8. build plus estimate time
        )

        write_regression_results(
            first_line_comment=first_comment,
            second_line=second,
            third_line=third,
            timing_type="MILLISECONDS",
            output_path=results_path,
            estimator_name=regressor_name,
            resample_id=resample_id,
            y_pred=train_preds,
            dataset_name=dataset_name,
            y_true=y_train,
            split="TRAIN",
            full_path=False,
        )


def load_and_run_regression_experiment(
    problem_path,
    results_path,
    dataset,
    regressor,
    resample_id=0,
    regressor_name=None,
    overwrite=False,
    build_train_file=False,
    predefined_resample=False,
):
    """Load a dataset and run a classification experiment.

    Method to run a basic experiment and write the results to files called
    testFold<resampleID>.csv and, if required, trainFold<resampleID>.csv.

    Parameters
    ----------
    problem_path : str
        Location of problem files, full path.
    results_path : str
        Location of where to write results. Any required directories will be created.
    dataset : str
        Name of problem. Files must be  <problem_path>/<dataset>/<dataset>+"_TRAIN.ts",
        same for "_TEST".
    regressor : BaseClassifier
        Classifier to be used in the experiment, if none is provided one is selected
        using cls_name using resample_id as a seed.
    regressor_name : str, default = None
        Name of classifier used in writing results. If none the name is taken from
        the classifier
    resample_id : int, default=0
        Seed for resampling. If set to 0, the default train/test split from file is
        used. Also used in output file name.
    overwrite : bool, default=False
        If set to False, this will only build results if there is not a result file
        already present. If True, it will overwrite anything already there.
    build_train_file : bool, default=False
        Whether to generate train files or not. If true, it performs a 10-fold
        cross-validation on the train data and saves. If the classifier can produce its
        own estimates, those are used instead.
    predefined_resample : bool, default=False
        Read a predefined resample from file instead of performing a resample. If True
        the file format must include the resample_id at the end of the dataset name i.e.
        <problem_path>/<dataset>/<dataset>+<resample_id>+"_TRAIN.ts".
    """
    if regressor_name is None:
        regressor_name = type(regressor).__name__

    build_test_file, build_train_file = _check_existing_results(
        results_path,
        regressor_name,
        dataset,
        resample_id,
        overwrite,
        True,
        build_train_file,
    )

    if not build_test_file and not build_train_file:
        warnings.warn("All files exist and not overwriting, skipping.")
        return

    X_train, y_train, X_test, y_test, resample = _load_data(
        problem_path, dataset, resample_id, predefined_resample
    )

    if resample:
        X_train, y_train, X_test, y_test = resample_data(
            X_train, y_train, X_test, y_test, random_state=resample_id
        )

    # Ensure labels are floats
    y_train = y_train.astype(float)
    y_test = y_test.astype(float)

    run_regression_experiment(
        X_train,
        y_train,
        X_test,
        y_test,
        regressor,
        results_path,
        regressor_name=regressor_name,
        dataset_name=dataset,
        resample_id=resample_id,
        build_test_file=build_test_file,
        build_train_file=build_train_file,
    )


def run_clustering_experiment(
    X_train,
    y_train,
    clusterer,
    results_path,
    X_test=None,
    y_test=None,
    clusterer_name="",
    dataset_name="",
    resample_id=None,
    build_test_file=False,
    build_train_file=True,
):
    """
    Run a clustering experiment and save the results to file.

    Method to run a basic experiment and write the results to files called
    testFold<resampleID>.csv and, if required, trainFold<resampleID>.csv. This
    version loads the data from file based on a path. The clusterer is always trained on
    the required input data trainX. Output to trainResample<resampleID>.csv will be
    the predicted clusters of trainX. If trainY is also passed, these are written to
    file. If the clusterer makes probabilistic predictions, these are also written to
    file. See write_results_to_uea_format for more on the output. Be warned,
    this method will always overwrite existing results, check bvefore calling or use
    load_and_run_clustering_experiment instead.

    Parameters
    ----------
    X_train : pd.DataFrame or np.array
        The data to cluster.
    clusterer : BaseClusterer
        The clustering object
    results_path : str
        Where to write the results to
    y_train : np.array, default = None
        Train data tue class labels, only used for file writing, ignored by the
        clusterer
    X_test : pd.DataFrame or np.array, default = None
        Test attribute data, if present it is used for predicting testY
    y_test : np.array, default = None
        Test data true class labels, only used for file writing, ignored by the
        clusterer
    clusterer_name : str, default = None
        Name of the clusterer, written to the results file, ignored if None
    dataset_name : str, default = None
        Name of problem, written to the results file, ignored if None
    resample_id : int, default = 0
        Resample identifier, defaults to 0

    """
    if not build_test_file and not build_train_file:
        raise Exception(
            "Both test_file and train_file are set to False. "
            "At least one must be written."
        )

    le = preprocessing.LabelEncoder()
    y_train = le.fit_transform(y_train)
    if y_test is not None:
        y_test = le.transform(y_test)

    encoder_dict = {label: i for i, label in enumerate(le.classes_)}
    n_classes = len(np.unique(y_train))

    start = int(round(time.time() * 1000))
    clusterer.fit(X_train)
    fit_time = int(round(time.time() * 1000)) - start

    first_comment = (
        "Generated by clustering_experiments.py on "
        f"{datetime.now().strftime('%m/%d/%Y, %H:%M:%S')}. "
        f"Encoder dictionary: {str(encoder_dict)}"
    )

    second = str(clusterer.get_params())
    second.replace("\n", " ")
    second.replace("\r", " ")

    if build_train_file:
        start = int(round(time.time() * 1000))
        train_probs = clusterer.predict_proba(X_train)
        train_time = int(round(time.time() * 1000)) - start

        train_preds = clusterer.classes_[np.argmax(train_probs, axis=1)]
        train_acc = clustering_accuracy(y_train, train_preds)
        third = (
            f"{train_acc},"  # 1. clustering accuracy
            f"{fit_time},"  # 2. fit time
            f"{train_time},"  # 3. predict time
            "-1,-1,"  # 4. benchmark time, 5. memory usage
            f"{n_classes},"  # 6. number of classes
            f"{len(train_probs[0])}"  # 7. number of clusters
        )

        write_clustering_results(
            first_line_comment=first_comment,
            second_line=second,
            third_line=third,
            timing_type="MILLISECONDS",
            output_path=results_path,
            estimator_name=clusterer_name,
            resample_id=resample_id,
            y_pred=train_preds,
            predicted_probs=train_probs,
            dataset_name=dataset_name,
            y_true=y_train,
            split="TRAIN",
            full_path=False,
        )

    if build_test_file:
        if X_test is None or y_test is None:
            raise Exception("Test data not provided, cannot build test file.")

        start = int(round(time.time() * 1000))
        test_probs = clusterer.predict_proba(X_test)
        test_time = int(round(time.time() * 1000)) - start

        test_preds = clusterer.classes_[np.argmax(test_probs, axis=1)]
        test_acc = clustering_accuracy(y_train, test_preds)
        third = (
            f"{test_acc},"  # 1. clustering accuracy
            f"{fit_time},"  # 2. fit time
            f"{test_time},"  # 3. predict time
            "-1,-1,"  # 4. benchmark time, 5. memory usage
            f"{n_classes},"  # 6. number of classes
            f"{len(test_probs[0])}"  # 7. number of clusters
        )

        write_clustering_results(
            first_line_comment=first_comment,
            second_line=second,
            third_line=third,
            timing_type="MILLISECONDS",
            output_path=results_path,
            estimator_name=clusterer_name,
            resample_id=resample_id,
            y_pred=test_preds,
            predicted_probs=test_probs,
            dataset_name=dataset_name,
            y_true=y_test,
            split="TEST",
            full_path=False,
        )


def load_and_run_clustering_experiment(
    problem_path,
    results_path,
    dataset,
    clusterer,
    resample_id=0,
    clusterer_name=None,
    overwrite=False,
    build_test_file=False,
    predefined_resample=False,
):
    """Run a clustering experiment.

    Method to run a basic experiment and write the results to files called
    testFold<resampleID>.csv and, if required, trainFold<resampleID>.csv. This
    version loads the data from file based on a path. The
    clusterer is always trained on the

    Parameters
    ----------
    problem_path : str
        Location of problem files, full path.
    results_path : str
        Location of where to write results. Any required directories will be created
    dataset : str
        Name of problem. Files must be  <problem_path>/<dataset>/<dataset>+
        "_TRAIN"+format, same for "_TEST"
    clusterer : the clusterer
    clusterer_name : str, default =None
        determines what to call the write directory. If None, it is set to
        type(clusterer).__name__
    resample_id : int, default = 0
        Seed for resampling. If set to 0, the default train/test split from file is
        used. Also used in output file name.
    overwrite : boolean, default = False
        if False, this will only build results if there is not a result file already
        present. If True, it will overwrite anything already there.
    format: string, default = ".ts"
        Valid formats are ".ts", ".arff", ".tsv" and ".long". For more info on
        format, see   examples/loading_data.ipynb
    build_train_file: boolean, default = False
        whether to generate train files or not. If true, it performs a 10xCV on the
        train and saves
    """
    if clusterer_name is None:
        clusterer_name = type(clusterer).__name__

    build_test_file, build_train_file = _check_existing_results(
        results_path,
        clusterer_name,
        dataset,
        resample_id,
        overwrite,
        build_test_file,
        True,
    )

    if not build_test_file and not build_train_file:
        warnings.warn("All files exist and not overwriting, skipping.")
        return

    X_train, y_train, X_test, y_test, resample = _load_data(
        problem_path, dataset, resample_id, predefined_resample
    )

    if resample:
        X_train, y_train, X_test, y_test = stratified_resample_data(
            X_train, y_train, X_test, y_test, random_state=resample_id
        )

    run_clustering_experiment(
        X_train,
        y_train,
        clusterer,
        results_path,
        X_test=X_test,
        y_test=y_test,
        clusterer_name=clusterer_name,
        dataset_name=dataset,
        resample_id=resample_id,
        build_train_file=build_train_file,
        build_test_file=build_test_file,
    )


def _check_existing_results(
    results_path,
    estimator_name,
    dataset,
    resample_id,
    overwrite,
    build_test_file,
    build_train_file,
):
    if not overwrite:
        resample_str = "" if resample_id is None else str(resample_id)

        if build_test_file:
            full_path = (
                f"{results_path}/{estimator_name}/Predictions/{dataset}/"
                f"/testResample{resample_str}.csv"
            )

            if os.path.exists(full_path):
                build_test_file = False

        if build_train_file:
            full_path = (
                f"{results_path}/{estimator_name}/Predictions/{dataset}/"
                f"/trainResample{resample_str}.csv"
            )

            if os.path.exists(full_path):
                build_train_file = False

        return build_test_file, build_train_file
    else:
        return True, True


def _load_data(problem_path, dataset, resample_id, predefined_resample):
    if resample_id is not None and predefined_resample:
        resample_str = "" if resample_id is None else str(resample_id)

        X_train, y_train = load_ts(
            f"{problem_path}/{dataset}/{dataset}{resample_str}_TRAIN.ts"
        )
        X_test, y_test = load_ts(
            f"{problem_path}/{dataset}/{dataset}{resample_str}_TEST.ts"
        )

        resample_data = False
    else:
        X_train, y_train = load_ts(f"{problem_path}/{dataset}/{dataset}_TRAIN.ts")
        X_test, y_test = load_ts(f"{problem_path}/{dataset}/{dataset}_TEST.ts")

        resample_data = False if resample_id == 0 and resample_id is None else True

    return X_train, y_train, X_test, y_test, resample_data
