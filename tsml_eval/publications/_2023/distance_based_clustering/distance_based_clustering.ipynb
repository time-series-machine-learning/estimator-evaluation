{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# A Review and Evaluation of Elastic Distance Functions for Time Series Clustering\n",
    "\n",
    "Webpage and repo package to support the paper \"A Review and Evaluation of Elastic Distance Functions for Time Series Clustering\" submitted to Springer Knowledge and Information Systems (KAIS).\n",
    "\n",
    "[Notebook for creating alignment figures.](./alignment_and_paths_figures.ipynb)\n",
    "\n",
    "[Timing comparison for Python distance implementations.](./package_distance_timing.ipynb)\n",
    "\n",
    "This page is a work in progress, and will be tidied up in due course.\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Time series clustering is the act of grouping time series data without recourse to a label. Algorithms that cluster time series can be classified into two groups: those that employ a time series specific distance measure; and those that derive features from time series. Both approaches usually rely on traditional clustering algorithms such as k-means. Our focus is on distance based time series that employ elastic distance measures, i.e. distances that perform some kind of realignment whilst measuring distance. We describe nine commonly used elastic distance measures and compare their performance with k-means and k-medoids clustering. Our findings are surprising. The most popular technique, dynamic time warping (DTW), performs worse than Euclidean distance with k-means, and even when tuned, is no better. Using k-medoids rather than k-means improved the clusterings for all nine distance measures. DTW is not significantly better than Euclidean distance with k-medoids. Generally, distance measures that employ editing in conjunction with warping perform better, and one distance measure, the move-split-merge (MSM) method, is the best performing measure of this study. We also compare to clustering with DTW using barycentre averaging (DBA). We find that DBA does improve DTW k-means, but that the standard DBA is still worse than using MSM. Our conclusion is to recommend MSM with k-medoids as the benchmark algorithm for clustering time series with elastic distance measures. We provide implementations, results and guidance on reproducing results on the associated GitHub repository.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from aeon.benchmarking.experiments import run_clustering_experiment\n",
    "from aeon.clustering.k_means import TimeSeriesKMeans\n",
    "from aeon.datasets import load_acsf1\n",
    "\n",
    "X_train, y_train = load_acsf1(split=\"train\")\n",
    "X_test, y_test = load_acsf1(split=\"test\")\n",
    "\n",
    "k_means_clusterer = TimeSeriesKMeans(\n",
    "    n_clusters=10,  # Set to 10 as num classes is 10\n",
    "    metric=\"euclidean\",\n",
    ")\n",
    "\n",
    "run_clustering_experiment(\n",
    "    X_train,\n",
    "    k_means_clusterer,\n",
    "    results_path=\"./example-notebook-results\",\n",
    "    trainY=y_train,\n",
    "    testX=X_test,\n",
    "    testY=y_test,\n",
    "    cls_name=\"kmeans\",\n",
    "    dataset_name=\"acsf1\",\n",
    "    resample_id=0,\n",
    "    overwrite=False,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
