{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Evaluation of raw results</h1>\n",
    "\n",
    "To evaluate the results there are two states results can be in:\n",
    "1) 'Raw' - the results are not yet processed.\n",
    "2) 'Metric results' - the raw have been turned into metric results over n folds.\n",
    "\n",
    "This notebook looks at 'raw', see evaluation_metric_results.ipynb for metric results\n",
    "\n",
    "<h2> Raw results </h2>\n",
    "Raw result are those yet to be processed. A result for an estimator in the 'raw' format\n",
    "is a csv that takes the following format:\n",
    "\n",
    "<b>File name</b>: \\<split>Resample\\<resample number>.csv e.g. testResample0.csv\n",
    "<b>First line</b>: \\<dataset name>,\\<estimator name>,\\<split>,\\<run time value>,\\<run time unit>,\\<extra info>\n",
    "<b>Second line</b>: \\<dict containing parameters passed to estimator>\n",
    "<b>Third line</b>: \\<TBD>\n",
    "<b>Fourth line and onwards</b>: \\<True y class>,\\<predicted y class>,\\<num classes>,<proba class 1>,<proba class 2>,...,\\<proba class n>\n",
    "\n",
    "<h3>Example file</h3>\n",
    "See notebooks/example_data/testResample0.csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First line: ['ACSF1', 'kmeans-ddtw', 'test', '0', 'MILLISECONDS', 'Generated by clustering_experiments on 2022-05-01 16:30:44.347516']\n",
      "\n",
      "Second line: [\"{'average_params': {'averaging_distance_metric': 'ddtw'}\", \" 'averaging_method': 'mean'\", \" 'distance_params': {'window': 1.0\", \" 'epsilon': 0.05\", \" 'g': 0.05\", \" 'c': 1}\", \" 'init_algorithm': 'random'\", \" 'max_iter': 300\", \" 'metric': 'ddtw'\", \" 'n_clusters': 10\", \" 'n_init': 10\", \" 'random_state': 1\", \" 'tol': 1e-06\", \" 'verbose': False}\"]\n",
      "\n",
      "Third line: ['0', '19307', '19283', '-1', '-1', '10', '10']\n",
      "\n",
      "Results:    0  1  2    3    4    5    6    7    8    9    10   11   12\n",
      "0   9  9     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
      "1   9  9     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
      "2   9  9     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
      "3   9  9     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
      "4   9  9     0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
      ".. .. .. ..  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
      "95  1  0     1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "96  1  0     1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "97  1  0     1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "98  1  4     0.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  0.0  0.0\n",
      "99  1  0     1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
      "\n",
      "[100 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "line_one = None\n",
    "line_two = None\n",
    "line_three = None\n",
    "results = []\n",
    "with open(\"./example_data/testResample0.csv\", newline=\"\\n\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=\",\")\n",
    "    line_one = next(reader)\n",
    "    line_two = next(reader)\n",
    "    line_three = next(reader)\n",
    "    for row in reader:\n",
    "        results.append(row)\n",
    "\n",
    "print(f\"First line: {line_one}\\n\")\n",
    "print(f\"Second line: {line_two}\\n\")\n",
    "print(f\"Third line: {line_three}\\n\")\n",
    "print(f\"Results: {pd.DataFrame(results)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<h1>Evaluating raw results</h1>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating estimator:  kmeans_dba\n",
      "----> evaluating experiment:  kmeans-dtw\n",
      "----> evaluating experiment:  kmeans-msm\n"
     ]
    }
   ],
   "source": [
    "from sktime_estimator_evaluation.evaluation import (\n",
    "    CLUSTER_METRIC_CALLABLES,\n",
    "    evaluate_raw_results,\n",
    ")\n",
    "\n",
    "results = evaluate_raw_results(\n",
    "    experiment_name=\"example_experiment_name\",  # The name of the experiment\n",
    "    path=\"./example_data/clustering_results\",  # Path to the raw files\n",
    "    metrics=CLUSTER_METRIC_CALLABLES,  # List of metrics to evaluate the results with\n",
    ")\n",
    "# The return value will take the format of:\n",
    "#     List[Dict]\n",
    "#         A list of metric results. Each metric will take the form:\n",
    "#         {\n",
    "#             'metric_name': str,\n",
    "#             'test_estimator_results': [\n",
    "#                 {\n",
    "#                     'estimator_name': str,\n",
    "#                     'result': pd.DataFrame\n",
    "#                 },\n",
    "#                 {\n",
    "#                     'estimator_name': str,\n",
    "#                     'result': pd.DataFrame\n",
    "#                 }\n",
    "#             ],\n",
    "#             'train_estimator_results': [\n",
    "#                 {\n",
    "#                     'estimator_name': str,\n",
    "#                     'result': pd.DataFrame\n",
    "#                 },\n",
    "#                 {\n",
    "#                     'estimator_name': str,\n",
    "#                     'result': pd.DataFrame\n",
    "#                 }\n",
    "#             ],\n",
    "#         }\n",
    "#         Each result dataframe will be in the following format:\n",
    "#         | folds    | 0   | 1   | 2   | 3   | ... |\n",
    "#         ------------------------------------------\n",
    "#         | dataset1 | 0.0 | 0.0 | 0.0 | 0.0 | ... |\n",
    "#         | dataset2 | 0.0 | 0.0 | 0.0 | 0.0 | ... |\n",
    "#         | dataset3 | 0.0 | 0.0 | 0.0 | 0.0 | ... |\n",
    "#         | dataset4 | 0.0 | 0.0 | 0.0 | 0.0 | ... |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating estimator:  clustering_results\n",
      "----> evaluating experiment:  kmeans-dtw\n",
      "----> evaluating experiment:  kmeans-euclidean\n"
     ]
    }
   ],
   "source": [
    "# If we want to write the results to a file with can specify an output path.\n",
    "results_output = evaluate_raw_results(\n",
    "    experiment_name=\"example_experiment_name\",  # The name of the experiment\n",
    "    path=\"./example_data/clustering_results\",  # Path to the raw files\n",
    "    metrics=CLUSTER_METRIC_CALLABLES,  # List of metrics to evaluate the results with\n",
    "    output_dir=\"./example_data/output/clustering_results\",  # Path to the output directory\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating estimator:  clustering_results\n",
      "----> evaluating experiment:  kmeans-dtw\n",
      "----> evaluating experiment:  kmeans-euclidean\n"
     ]
    }
   ],
   "source": [
    "# If we want to use a custom metric we can specify it in the metrics list.\n",
    "results_str_metric = evaluate_raw_results(\n",
    "    experiment_name=\"example_experiment_name\",  # The name of the experiment\n",
    "    path=\"./example_data/clustering_results\",  # Path to the raw files\n",
    "    metrics=[\"ACC\", \"RI\", \"AMI\"],  # List of metrics to evaluate the results with\n",
    ")\n",
    "\n",
    "# The full list of available clustering metrics is:\n",
    "# RI (rand index), AMI (adjusted mutual information), ACC (accuracy), NMI (normalized mutual information), ARI (adjusted rand index), MI (mutual information)\n",
    "valid_cluster_metrics = [\"RI\", \"AMI\", \"ACC\", \"NMI\", \"ARI\", \"MI\"]\n",
    "# The full list of available classification metrics is:\n",
    "# ACC (accuracy), F1 (f1 score), Precision (precision score), Recall (recall score), Jacard (jaccard score), ROC_AUC (roc auc score), Brier (brier score), Log_Loss (log loss score) Balanced_Accuracy (balanced accuracy score), Top_k_Accuracy (top k accuracy score), Average_Precision (average precision score)\n",
    "valid_classification_metrics = [\n",
    "    \"ACC\",\n",
    "    \"F1\",\n",
    "    \"Precision\",\n",
    "    \"Recall\",\n",
    "    \"Jacard\",\n",
    "    \"ROC_AUC\",\n",
    "    \"Brier\",\n",
    "    \"Log_Loss\",\n",
    "    \"Balanced_Accuracy\",\n",
    "    \"Top_k_Accuracy\",\n",
    "    \"Average_Precision\",\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating estimator:  clustering_results\n",
      "----> evaluating experiment:  kmeans-dtw\n",
      "----> evaluating experiment:  kmeans-euclidean\n",
      "[{'metric_name': 'custom_NMI', 'test_estimator_results': [{'estimator_name': 'kmeans-dtw', 'result':        folds         0         1         2         3         4         5  \\\n",
      "0      ACSF1  0.471553  0.511391  0.496259  0.503164  0.536588  0.519174   \n",
      "1      Adiac  0.687087  0.671689  0.689086  0.699429  0.693124  0.697099   \n",
      "2  ArrowHead  0.271769  0.247246  0.247246  0.247246  0.247246  0.247246   \n",
      "\n",
      "          6         7         8  ...        20        21        22        23  \\\n",
      "0  0.513126  0.486407  0.498182  ...  0.553775  0.510917  0.521075  0.502281   \n",
      "1  0.705582  0.705026  0.693871  ...  0.698392  0.698325  0.703904  0.684360   \n",
      "2  0.271769  0.247246  0.278141  ...  0.278141  0.247246  0.278141  0.278141   \n",
      "\n",
      "         24        25        26        27        28        29  \n",
      "0  0.498182  0.504014  0.488849  0.455637  0.480972  0.468552  \n",
      "1  0.700289  0.676287  0.701198  0.691760  0.683737  0.693983  \n",
      "2  0.278141  0.247246  0.278141  0.247246  0.278141  0.278141  \n",
      "\n",
      "[3 rows x 31 columns]}, {'estimator_name': 'kmeans-euclidean', 'result':        folds         0\n",
      "0      ACSF1  0.464958\n",
      "1      Adiac  0.649323\n",
      "2  ArrowHead  0.375035}], 'train_estimator_results': [{'estimator_name': 'kmeans-dtw', 'result':        folds         0         1         2         3         4         5  \\\n",
      "0      ACSF1  0.479524  0.462586  0.484659  0.480955  0.458070  0.529013   \n",
      "1      Adiac  0.688464  0.671293  0.679770  0.684926  0.681067  0.701316   \n",
      "2  ArrowHead  0.423304  0.348971  0.348971  0.348971  0.348971  0.348971   \n",
      "\n",
      "          6         7         8  ...        20        21        22        23  \\\n",
      "0  0.509389  0.467551  0.464877  ...  0.489037  0.525760  0.499311  0.435441   \n",
      "1  0.693248  0.683764  0.690632  ...  0.686026  0.694562  0.684769  0.682722   \n",
      "2  0.423304  0.348971  0.466122  ...  0.466122  0.348971  0.466122  0.466122   \n",
      "\n",
      "         24        25        26        27        28        29  \n",
      "0  0.454772  0.523863  0.520076  0.458638  0.500850  0.517481  \n",
      "1  0.687143  0.682815  0.691376  0.677762  0.686258  0.689476  \n",
      "2  0.466122  0.348971  0.466122  0.348971  0.466122  0.466122  \n",
      "\n",
      "[3 rows x 31 columns]}, {'estimator_name': 'kmeans-euclidean', 'result':        folds         0\n",
      "0      ACSF1  0.507081\n",
      "1      Adiac  0.661040\n",
      "2  ArrowHead  0.562550}]}, {'metric_name': 'custom_dict_NMI', 'test_estimator_results': [{'estimator_name': 'kmeans-dtw', 'result':        folds         0         1         2         3         4         5  \\\n",
      "0      ACSF1  0.471553  0.511391  0.496259  0.503164  0.536588  0.519174   \n",
      "1      Adiac  0.687087  0.671689  0.689086  0.699429  0.693124  0.697099   \n",
      "2  ArrowHead  0.271769  0.247246  0.247246  0.247246  0.247246  0.247246   \n",
      "\n",
      "          6         7         8  ...        20        21        22        23  \\\n",
      "0  0.513126  0.486407  0.498182  ...  0.553775  0.510917  0.521075  0.502281   \n",
      "1  0.705582  0.705026  0.693871  ...  0.698392  0.698325  0.703904  0.684360   \n",
      "2  0.271769  0.247246  0.278141  ...  0.278141  0.247246  0.278141  0.278141   \n",
      "\n",
      "         24        25        26        27        28        29  \n",
      "0  0.498182  0.504014  0.488849  0.455637  0.480972  0.468552  \n",
      "1  0.700289  0.676287  0.701198  0.691760  0.683737  0.693983  \n",
      "2  0.278141  0.247246  0.278141  0.247246  0.278141  0.278141  \n",
      "\n",
      "[3 rows x 31 columns]}, {'estimator_name': 'kmeans-euclidean', 'result':        folds         0\n",
      "0      ACSF1  0.464958\n",
      "1      Adiac  0.649323\n",
      "2  ArrowHead  0.375035}], 'train_estimator_results': [{'estimator_name': 'kmeans-dtw', 'result':        folds         0         1         2         3         4         5  \\\n",
      "0      ACSF1  0.479524  0.462586  0.484659  0.480955  0.458070  0.529013   \n",
      "1      Adiac  0.688464  0.671293  0.679770  0.684926  0.681067  0.701316   \n",
      "2  ArrowHead  0.423304  0.348971  0.348971  0.348971  0.348971  0.348971   \n",
      "\n",
      "          6         7         8  ...        20        21        22        23  \\\n",
      "0  0.509389  0.467551  0.464877  ...  0.489037  0.525760  0.499311  0.435441   \n",
      "1  0.693248  0.683764  0.690632  ...  0.686026  0.694562  0.684769  0.682722   \n",
      "2  0.423304  0.348971  0.466122  ...  0.466122  0.348971  0.466122  0.466122   \n",
      "\n",
      "         24        25        26        27        28        29  \n",
      "0  0.454772  0.523863  0.520076  0.458638  0.500850  0.517481  \n",
      "1  0.687143  0.682815  0.691376  0.677762  0.686258  0.689476  \n",
      "2  0.466122  0.348971  0.466122  0.348971  0.466122  0.466122  \n",
      "\n",
      "[3 rows x 31 columns]}, {'estimator_name': 'kmeans-euclidean', 'result':        folds         0\n",
      "0      ACSF1  0.507081\n",
      "1      Adiac  0.661040\n",
      "2  ArrowHead  0.562550}]}, {'metric_name': 'ACC', 'test_estimator_results': [{'estimator_name': 'kmeans-dtw', 'result':        folds        0         1         2         3         4         5  \\\n",
      "0      ACSF1  0.12000  0.140000  0.050000  0.020000  0.130000  0.060000   \n",
      "1      Adiac  0.02046  0.051151  0.038363  0.030691  0.048593  0.046036   \n",
      "2  ArrowHead  0.08000  0.297143  0.188571  0.371429  0.297143  0.114286   \n",
      "\n",
      "          6        7         8  ...        20        21        22        23  \\\n",
      "0  0.140000  0.06000  0.000000  ...  0.110000  0.010000  0.150000  0.210000   \n",
      "1  0.015345  0.01023  0.030691  ...  0.035806  0.071611  0.040921  0.015345   \n",
      "2  0.371429  0.44000  0.542857  ...  0.148571  0.371429  0.542857  0.428571   \n",
      "\n",
      "         24        25        26        27        28        29  \n",
      "0  0.110000  0.010000  0.050000  0.030000  0.080000  0.120000  \n",
      "1  0.025575  0.005115  0.071611  0.015345  0.002558  0.015345  \n",
      "2  0.360000  0.440000  0.428571  0.188571  0.542857  0.148571  \n",
      "\n",
      "[3 rows x 31 columns]}, {'estimator_name': 'kmeans-euclidean', 'result':        folds         0\n",
      "0      ACSF1  0.140000\n",
      "1      Adiac  0.010230\n",
      "2  ArrowHead  0.165714}], 'train_estimator_results': [{'estimator_name': 'kmeans-dtw', 'result':        folds         0         1         2         3         4         5  \\\n",
      "0      ACSF1  0.120000  0.120000  0.030000  0.010000  0.150000  0.050000   \n",
      "1      Adiac  0.023077  0.051282  0.041026  0.064103  0.051282  0.069231   \n",
      "2  ArrowHead  0.111111  0.305556  0.194444  0.583333  0.305556  0.111111   \n",
      "\n",
      "          6         7         8  ...        20        21        22        23  \\\n",
      "0  0.130000  0.090000  0.010000  ...  0.100000  0.020000  0.140000  0.190000   \n",
      "1  0.023077  0.007692  0.038462  ...  0.033333  0.048718  0.035897  0.010256   \n",
      "2  0.611111  0.222222  0.500000  ...  0.055556  0.583333  0.500000  0.305556   \n",
      "\n",
      "         24        25        26        27        28        29  \n",
      "0  0.100000  0.000000  0.030000  0.040000  0.080000  0.110000  \n",
      "1  0.012821  0.012821  0.076923  0.041026  0.002564  0.012821  \n",
      "2  0.416667  0.222222  0.305556  0.194444  0.500000  0.055556  \n",
      "\n",
      "[3 rows x 31 columns]}, {'estimator_name': 'kmeans-euclidean', 'result':        folds         0\n",
      "0      ACSF1  0.150000\n",
      "1      Adiac  0.010256\n",
      "2  ArrowHead  0.055556}]}]\n"
     ]
    }
   ],
   "source": [
    "# Custom metric or custom parameters to existing metrics\n",
    "# A custom metric function can only have two parameters: true labels and predicted labels.\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sktime_estimator_evaluation.evaluation import MetricCallable\n",
    "\n",
    "\n",
    "# In this example we want to use normalized mutual info score but pass additional\n",
    "# parameters so you wrap it in a function like so:\n",
    "def example_custom_NMI(true_labels, predicted_labels):\n",
    "    return normalized_mutual_info_score(\n",
    "        true_labels, predicted_labels, average_method=\"min\"\n",
    "    )\n",
    "\n",
    "\n",
    "# We then construct a custom MetricCallable object OR dict.\n",
    "custom_metric = MetricCallable(name=\"custom_NMI\", callable=example_custom_NMI)\n",
    "dict_custom_metric = {\"name\": \"custom_dict_NMI\", \"callable\": example_custom_NMI}\n",
    "# Both are valid\n",
    "\n",
    "result_custom_metric = evaluate_raw_results(\n",
    "    experiment_name=\"example_experiment_name\",\n",
    "    path=\"./example_data/clustering_results\",\n",
    "    metrics=[\n",
    "        custom_metric,\n",
    "        dict_custom_metric,\n",
    "        \"ACC\",\n",
    "    ],  # We can still use other metrics\n",
    ")\n",
    "\n",
    "print(result_custom_metric)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
